Across the 218 high-severity samples, gpt-oss-120b’s core failure looks less like “bad wording” and more like a weak internal scene ledger. It generates locally striking lines, but it does not keep persistent, constraint-checked state for objects, locations, time, and referents. The result is continuity collapse right where readers most need reliability: in climaxes built around a symbolic prop and in fast “ritual beats” where the model summarizes instead of staging. You see it when an object’s state flips mid-action, as in “She sealed the vial with a click…” followed shortly by “She tucked the empty vial into her coat…,” or when an item is removed from the world and then casually pocketed: “The key… melted into the stone…” and later “She tucked the key… into the folds of her robe…”. The same mechanism shows up in mundane mystery handling: “Inside lay a tiny locket photo… and a handwritten note…” then later “Together they opened the envelope” again, as if the story can restart the reveal beat without reconciling what already happened. This model’s decoder seems to prioritize the next vivid image and a sense of motif payoff over reconciling previously asserted physical facts, so props become thematic tokens rather than tracked inventory.

The same state-tracking weakness appears at the level of space and time. Settings are treated like interchangeable mood stages, so the narrative jump-cuts between incompatible spatial conditions without depicting traversal, especially when the model wants a “bigger” backdrop for the next beat. A ship can be “beneath the harbor’s deepest trench” and yet its “hull… broke the surface,” or a character can be underwater with “his breath a silent bubble” and then “let the sea carry his quiet gratitude to the stars.” These aren’t just “surreal” choices; they break the reader’s ability to build a stable scene map because the text asserts mutually exclusive physical states as if they were adjacent camera angles. Time gets used the same way—as atmosphere paint rather than a constraint system—so a scene slides from “Tonight, the waning crescent…” to “at dawn, when the moon would rise again in its full glory.” Once time anchors are unstable, any plot that depends on “at dawn,” “before,” or “after” loses its causal spine, and the reader stops believing that actions are happening in a shared, continuous world.

When the story needs a mechanism—how something works, why a plan succeeds, why a political or technical barrier falls—gpt-oss-120b often substitutes high-status abstraction for operational causality. It will assert a pseudo-rule like “microscopic patterns of such dust could preprocess reality” or build a “technical” chain that collapses under inspection: “a telescope that measured escape velocity” leading to “granting a burst of thrust.” The pattern is that explanation tokens (“protocol,” “algorithm,” “calibrated angle,” “preprocess”) function as rhetorical glue, not as simulated steps with constraints, costs, and intermediate observables. That’s why locked-container plots routinely fail at the hinge point: “a small, locked mailbox…” immediately followed by “Inside… lay an aged locket…,” with the opening action simply omitted. The model is good at declaring that a solution is inevitable (“with the exchange fulfilled…”) and at emitting the aesthetic shape of a mechanism, but it does not consistently instantiate a rule-set that could be followed, tested, or even pictured.

Style pressure then amplifies the underlying world-model gaps. This model has a strong poetic-compression bias, so it reaches for paradox, synesthesia, and personification as default texture, often without a “literal gate” that marks the line as metaphor. The prose stacks oxymorons—“the air turned to icy warmth…” and “a lingering taste of cold fire”—until the reader can’t tell what is physically happening versus what is ornamental phrasing. It also generates category errors that read like broken physics rather than intentional lyricism, such as “measure the exact temperature of the surrounding silence.” Because these devices are deployed repetitively and at high intensity, they don’t just decorate; they overwrite the scene’s concrete layer, making it harder for the model itself to stay grounded. When metaphor is treated as executable action, the story’s causal chain becomes even easier to snap, because anything can be asserted as happening without needing a compatible physical setup.

Finally, when sentences become dense—especially in introductions and concept dumps—the model’s planning horizon shows up as syntactic and referent instability. It over-compresses worldbuilding into long noun piles and then fails to replan when grammar breaks, producing phrases like “There, a face blind portrait artist named Mara…” that leave even the protagonist’s defining trait ambiguous. Coreference drifts in the same way: role-nouns introduced as labels get reinterpreted as people, objects, or ideas depending on what the next line wants, as with “distress signal sender…”. And the model sometimes slips into unmotivated perspective changes, e.g., “thread a labyrinth with your own story,” which reads like a decoding hiccup where a generic second-person template overrides established third-person narration. These aren’t isolated “typos”; they’re consistent with a generator that is optimizing line-by-line for evocative continuation, but doesn’t reliably maintain a structured representation of who is present, what they’re holding, where they are, and what has already been established as impossible.