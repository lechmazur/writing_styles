mistral-large-2512’s worst failures come from a missing “world-state ledger” during generation: it writes each sentence as if it remembers the story’s motifs, but not the story’s physical facts. That produces high-severity continuity breaks where an object is rhetorically “destroyed” for drama, then immediately reused because the object-token is still salient. You see it in hard contradictions like “its dial snapped clean off” followed shortly by “Elias reached out and turned the dial,” and in vanishing props that remain available for description: “The tablet was gone. The runes had faded…”. The same pattern recurs with transformations that are treated as mood beats rather than state changes—items “crumble to dust” yet persist in later handling, or get “left” somewhere and reappear because the narrative wants the symbol back in the character’s hand. Mechanically, this looks like motif-recall overriding literal continuity: once an object has been marked as important (dial, photo, scrimshaw, feather), the model preferentially reintroduces it to maintain thematic cohesion, even when that reintroduction implies mutually exclusive states.

That motif-first bias also drives the “causality vacuum” failures: the model chooses a high-impact outcome, then drops in an asserted mechanism that sounds poetic or technical but doesn’t actually connect actions to results. The tell is how often the resolution is introduced with a narrative shove rather than a derivation—effects simply happen because the story needs closure. Burning a lamp yields “the equations on the page rearranged themselves…”, star discrepancies instantly “pointed to a date: … three days hence,” and dripping juice onto a phonograph somehow “record[ed] the sound of the fruit’s surrender.” These aren’t just “unrealistic”; they’re structurally unearned because intermediate, falsifiable steps are missing, so editors experience them as deus ex rather than speculative invention. Under tight word limits, the model appears to optimize for surprise and thematic click (“everything aligns”) while skipping the bridging reasoning that would constrain the magic/tech into a rule-following system.

A related failure mode is rule-of-reality drift: sensory language and physics constraints are not consistently enforced across adjacent beats, especially in exotic settings. The model uses vivid, default atmosphere cues (mist, smell, warmth, air) as mood shorthand, and they leak into scenes where they contradict the established environment. That’s how you get underwater passages that suddenly have “the air thick with the scent of salt and ozone,” or lunar scenes where cobalt chloride turns pink by “moisture from the air.” Time and temperature become generic “recentness” markers, producing impossibilities like “its jagged edge still warm from her grip” years later, or clothing “still warm” when the wearer “had long since dissolved.” The underlying mechanism is a weak constraint checker: figurative phrases are emitted as literal world-facts, and once emitted, they aren’t reconciled with earlier scene tags (UNDERWATER, MOON, BLINDNESS), so the reader can’t tell whether the text is invoking magic or simply forgetting the rules.

When the model tries to “sound literary,” style pressure amplifies all of this by reducing semantic discipline. Purple-prose interference and garbled diction show up as associative chaining—one evocative token cues another—without a final coherence pass. The result is imagery that competes with itself (“like a blade of bottled lightning”) or collapses into category errors (“the sound of light breaking into silence”), and pseudo-elevated phrases that don’t map to any actionable meaning, such as “By waiting actively,” or “precisely playful.” Because these lines often land at decision points, they don’t merely feel florid; they obscure intent and block causal comprehension right when the story needs a concrete plan, movement, or constraint. This is also where the model’s technical-vocabulary bleed-through becomes visible (“progressive disclosure” used as a physical navigation method), which reads like the generator is selecting “smart-sounding” collocations rather than simulating a character doing something in a place.

Finally, the same compression bias produces unmarked scene cuts and payoff suppression. The model seems to plan in setpieces (a door bursts, an escape happens, a new tableau appears) but fails to emit the glue actions that make the jump legible, yielding teleportation like “…locked his apartment door behind him.” followed by “He stepped into the quiet courtyard…”, or “Then the chamber doors burst open…” immediately followed by an exterior lagoon path. And when it runs out of runway, it resolves with abstraction instead of showing the decisive beat, leaving endings that gesture at theme but don’t cash the plot, as in “…the texture of connection wasn’t something to be discovered.” Put together, these aren’t separate quirks; they’re coupled failure triggers. The more the story relies on symbolic props, twisty reveals, heightened lyricism, and rapid climactic escalation, the more mistral-large-2512 swaps state tracking and causal scaffolding for motif repetition and rhetorical closure—so the narrative feels like it’s composed of compelling sentences that refuse to agree on what is physically happening and why.