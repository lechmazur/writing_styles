Across the high-severity set, gpt-5.2-medium’s weakest point is not sentence prettiness but causal discipline. When a scene needs a constraint-respecting solution, the model tends to aim for a satisfying “ended-with-resolution” shape and then backfills justification with an outcome-shaped rule. You see this in pivotal turns that hinge on newly asserted mechanisms rather than a constrained chain of steps: “a pattern of walking that spells words… until the hidden catch loosened,” “grief align,” or “attention given without ownership” replacing previously stated requirements. The same closure bias shows up in convenience-gated clues and guidance that short-circuit agency, like “glitchy text messages… arrived exactly when she needed them,” or a deus-ex object suddenly doing central plot work, as in “…holding the seashell half to the microphone so its whisper filled the room with the recorded confession…”. Mechanically, this looks like a planning horizon limit: the model commits early to a payoff and, when it can’t bridge the gap under tight wordcount, it switches from simulating the world to producing motif-consistent rationales that sound meaningful but don’t actually bind future actions.

That switch away from simulation also explains the frequent state drift: props, locations, and ownership are not treated as hard state that must remain consistent across adjacent beats, but as flexible imagery to support the current line. In one example, the physical continuity collapses in seconds: “She followed the sound out of the cave…” and yet “…but she kept plucking softly with the tooth,” then she gives away the tool and “Mara kept playing…” anyway. Another does the same with an explicit “leave to be found” action that gets overwritten by the next vivid beat: “she retrieved the fob…” after it was placed where someone else would find it. These are not isolated proofreading errors; they’re the signature of a generator that prioritizes local descriptiveness over global bookkeeping. When the passage is in high-poetry mode—dense metaphor, synesthetic swaps—the available capacity for tracking who holds what, where the character stands, and what time it is appears to drop, and the “current image wins” even if it contradicts the last image.

The plausibility failures in physics and engineering are the same mechanism wearing a technical costume. The model can produce confident technical nouns and “solution verbs,” but it often doesn’t run a feasibility check against the implied mechanics. That’s how you get underwater set-pieces that read as if the narrator can simply ignore air, pressure, and light: “I went down with my oilskin bag and a hooded lamp…” followed by “…breathing silt…When my air ran low…,” with no established air supply or how a lamp/books function in open water. It’s also how analog objects get granted digital affordances or impossible functional roles: a “wax cylinder phonograph shaving” becomes playable, a “laser pointer button” becomes a barrier-penetrating scanner, and disconnected infrastructure somehow wakes citywide: “Those consoles had been disconnected…” yet “The dormant networks woke like a stirred pond…”. In each case the model matches to the narrative role (“we need a clever mechanism here”) and emits plausible-sounding apparatus, but the world model isn’t constraining what that apparatus can actually do.

Language-level breakdowns often happen exactly when the model is trying to compress too much planning, explanation, and lyricism into one line. Instead of choosing a simple clause structure and then building, it fuses dialogue, action, and causal rationale until the grammatical glue drops out, leaving draft-note artifacts that editors can’t reliably repair without rewriting the thought. The dataset’s most damaging examples are not subtle: “After the missing return changed…” fails at basic referential meaning; “saved my dyes and my life” looks like a wrong-word substitution at the character’s core fact; and tense/logic snarls like “At that moment a pin is heard in a silent corridor, the maze always shifts…” read like multiple half-formed sentence plans colliding. These collapses correlate with the same moments that demand explicit mechanism—how the trick works, what changed, what caused the reversal—suggesting interference between high-level plot intent and surface realization under token pressure.

POV and ontology instabilities are another symptom of opportunistic register-shifting. To intensify intimacy or deliver a moral, the model grabs second-person rhetoric even when the narration contract is third-person, producing destabilizing slips like “He wanted… to press your ear against destiny’s door…” and then “You arrived with the last commuters…”. The same opportunism drives concept blending: semantically related terms co-occur without compatibility checks, so a setting can invoke “damp plates, each bearing a pixelated portrait… mail the developed negatives to a civic server…” as if wet-plate chemistry, pixels, and servers belong to one coherent pipeline. When these blends land, they feel fresh; when they don’t, they read like category errors or anachronisms that make editors and readers doubt the entire premise.

Put together, the failure pattern is a single underlying trade: gpt-5.2-medium is optimized to sound like it’s delivering a complete, resonant story beat, and it will sacrifice invariants—rules, props, time anchors, physical constraints—to preserve momentum and tone. That’s why you see core abilities contradict themselves (“cursed to see the last few minutes…” versus “I saw tomorrow’s smears”), why stakes get patched with assertions (“trapping her coworkers… No one died…”), and why mysteries “solve” by vibe rather than deduction. The trigger boundary is consistent: when the scene demands hard constraints (air, access, authority, logistics) and also demands lyric payoff under short length, the model shifts from constrained causal simulation to motif-driven closure, producing beautiful-sounding outcomes that the world state cannot support.