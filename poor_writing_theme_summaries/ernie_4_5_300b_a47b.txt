ernie_4_5_300b_a47b’s worst failures come from weak persistence of story state across adjacent sentences: it does not reliably maintain a “ledger” of where objects are, what rules are in force, and what has already happened. Instead it optimizes for the next vivid or thematically charged line, even if that line silently overwrites the previous one. That is why climactic props routinely “teleport” or reappear after being consumed by the plot, as in “The broken arrow tip fused with the pedestal…” followed shortly by “As she flew home, the arrow tip cool in her pocket…”. The same pattern shows up in micro form when it commits to a decisive action and then keeps the prop alive for more beats because the prop is still narratively useful: “She knelt, pressing the seed into the soil.” becomes “The seed in her pocket grew warm,” as if the model is continuing a motif rather than honoring a completed physical action. Once readers notice these ledger violations, every later statement becomes suspect, so even strong imagery can’t rebuild trust.

A second connected mechanism is causal compression: when the story needs a multi-step chain (observation → reasoning → choice → constrained action → consequence), this model often emits a single high-salience “activation” gesture and lets the world rearrange itself afterward. That creates endings that feel like resets rather than payoffs, and it also forces retroactive contradictions because the story-state was never worked through. The same short-horizon behavior explains why it drops urgent premises without grounding them (“Before the possession completes…”) and why it resolves logistics by implication instead of events, producing effects that read like teleportation rather than plot. Under tight word budgets, this compression gets worse: the model tries to “land” an ending fast, so it skips the connective tissue that would preserve both causality and continuity, which then cascades into rule reversals and object drift.

Spatial and viewpoint continuity break for the same reason: the model treats adjacent paragraphs like film cuts, but doesn’t supply the linguistic equivalents of establishing shots, transitions, or a locked narrator. You can see the montage impulse in lines like “The elevator creaked upward…” followed by desk business, then “As the elevator lurched upward…” again, as if the scene is being re-queued each time it wants tension. The POV version is even more damaging because it destroys the reader’s model of who is perceiving events: “Eli opened his eyes. … made my chest ache.” slides into first person without any handoff, so the audience can’t tell whether the narrator changed or the text glitched. Once the “camera” is unstable, the model’s other mistakes (teleporting props, unexplained arrivals) stop feeling like isolated slips and start feeling like the story has no enforceable reality.

Environment constraint amnesia is the setting-level form of ledger drift: once an exotic constraint is introduced (underwater, vacuum, deep pressure), default “room template” prose takes over—air, doors, dripping ceilings, normal handling of objects—unless the constraint is reasserted every beat. That’s how you get “waded toward the submerged library.” followed by “Inside the ship, the air was thick with brine and decay.” and then a pristine interior, as if “submerged” was only a mood word. The diver example shows the same template override in action: “Mara unscrewed the ampoule… She drank…” reads like a terrestrial lab beat accidentally pasted into an underwater scene. This isn’t just a fact error; it’s a model-selection issue where familiar sensory language (“air,” “echo,” “gust”) is more probable than the constrained alternative, and the model doesn’t consistently run a plausibility check against the environment it already declared.

Finally, the model has a leaky separation between instruction-following and narrative mode, which is why “draft-room” artifacts appear at high severity: “Word count: 1290 (adjusted to fit 620-760 range…)” and even “(Oops—too long. Let’s trim.)” show the internal compliance stream escaping into the story. That leak is correlated with the same pressure conditions that trigger causal compression and jump-cuts (explicit length limits and rapid wrap-ups), and it reinforces the style-over-substance bias: the model can chase a “finished” cadence or a thematic aphorism even while basic bookkeeping collapses. The combined effect is writing that may sound moment-to-moment “literary,” but repeatedly fails the two things readers and editors rely on for coherence: stable state and earned change.