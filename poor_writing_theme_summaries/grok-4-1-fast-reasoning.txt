Grok-4-1-fast-reasoning’s worst writing failures come from a repeated pattern: when pressure rises (especially at endings and under hard length bounds), it optimizes for immediate intensity and closure rather than maintaining a stable “story world” with consistent state, viewpoint, and causality. You can see the control system breaking the fourth wall outright: “But wait, over. Need to trim? No, aim was 600-800…,” “Wait, word count way over…,” and the dead-on meta seal “Word count pulsed like the compass: complete… [Final count: 642]”. Mechanistically, this is planning/editing channel leakage: the model appears to keep constraint management active during generation, but doesn’t reliably gate it away from the narrative stream. The more the prompt forces tight bounds or a “land the ending now” move, the more those internal correction tokens (“Wait,” “No,” “adjust,” tally notes) get emitted as if they were story content, which instantly destroys reader trust because the text stops pretending to be a finished artifact and becomes a process log.

When it stays “in world,” the next major failure mode is skipped causality: setups jump to payoffs by pattern completion, as though symbolic resonance substitutes for mechanism. The prose gestures at discovery or method, then leaps straight to revelation without intermediate steps, costs, or constraints. That’s why wind becomes a perfect clue—“it wasn't random—the flutters mapped a secret… a fluttering cartography of hidden paths”—and why a blood daub turns into omniscient forensics: “Years later… he stole the evidence. The unified ribbon proved it…”. The same shortcut shows up in science-flavored handwaving chains like “It matched a formula for invisible ink… under UV-mimicking moonlight: coordinates…”. Under tight word budgets, it prefers a single “click” from clue to solution because that shape matches many story patterns; but without explicit inference steps, the reveal reads as arbitrary, and the reader experiences it as deus ex machina rather than intelligence or magic with rules.

A related mechanism is state tracking collapse under rapid escalation. Fast decoding with high novelty pressure keeps multiple competing continuations “alive,” and the model often fails to reconcile them before committing new details, so props and character states contradict themselves across adjacent lines. The action becomes untrustworthy in exactly the moments that need clarity: “Elara hurled the phone into a puddle, watching it spark and die…” followed shortly by “dialing the authority hotline…,” or the cast that gets dropped—“dropping the cast into the void”—and then reappears as leverage—“shoving him toward the cliff with the cast's weight”. Sometimes you can see the contradiction happen as an editorial fork left on-page: “He helped Harlan down to shelter below. No. As dawn bled gray, Elias descended alone.” This is the same gating failure as the word-count chatter, just expressed as narrative branching: revision tokens and alternative scene states leak into the canonical text, so the story cannot maintain a single authoritative timeline.

On the sentence level, the model often reaches for a “precision register” when confidence is low—pseudo-technical scaffolding, placeholder nouns, and academic qualifiers—to sound exact while actually becoming vaguer. That’s where you get template language like “This was the timeframe when context changes” and broken syntactic glue such as “preprocesses reality,” which reads like a half-inserted note rather than a finished clause. In parallel, it over-commits to compressed lyricism and novelty, stacking metaphors until they fight each other instead of composing. The profile’s oxymoron loops show up as sustained mixed imagery like “a futile rite against the sea's hushed blaze,” where repeating the contradiction is treated as intensification. Because the model is optimizing locally for striking phrasing, it reuses its own coined anchors and intensifiers even when they’ve stopped carrying meaning, which can slide into self-parody at climaxes: “Peace blossomed… Armistice complete… Eternal now… Fin.”

These failures connect because they are not separate “style problems,” but symptoms of the same control tradeoff: under constraint pressure, grok-4-1-fast-reasoning prioritizes momentum, novelty, and tidy closure over verification against prior text. It doesn’t consistently run three checks that human writers do implicitly: “Is this still inside the frame?” (channel separation), “Does this follow from what happened?” (causal chain), and “Can this still be true given earlier commitments?” (state/POV ledger). When any of those checks drop, the model compensates with rhetorical force—more metaphor, more pseudo-precision, more thesis-like certainty—so the prose can feel confident while the underlying story logic evaporates. The result is writing that can sound intense line-by-line but degrades sharply at the exact boundary conditions that matter most: endings, reveals, multi-prop action, and any prompt that demands both compression and coherence.