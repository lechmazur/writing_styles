gpt-5-medium pursues vividness before compatibility. Its local decoding strong-arms the next clause toward novelty and cadence, so figurative language accretes until the vehicles contradict. You see the failure in lines like “He felt anger erupt quietly, a volcano under water singing compliance.” and “I flip the chess clock toward it… like two blind fish.” The model prefers abstract-agency and lush images, but it has no semantic watchdog to reconcile mixed domains or internal negations, so oxymorons slip through as if they were craft. Register clashes amplify the damage: when technical or procedural frames appear, the lyric prior keeps driving, and metaphors start carrying physical claims they can’t support. Even simple temporal polarity is sacrificed to cadence, as in “Under a fleeting golden sunset, I arrived at the emerald station by moonlight, late but exactly on time.”

The same short planning horizon explains the model’s chronic prop and state flips. There is no persistent scene graph; definite descriptions get recycled as if they were the same object, and spatial constraints are dropped between beats. That’s why “The woman took her child and the cabinet…” is followed by “Alone, I opened the cabinet's back panel…,” and why “The lantern in my pack holds a single magic lantern slide, etched in copper with impossible filigree.” is immediately contradicted by “I unwrap the lantern and feed the slide into its mouth…”. The decoder smooths over these hard contradictions with fluent glue, but because there’s no ledger of who has what, where, and in what state, objects teleport, duplicate, or revert. The same mechanism produces impossible images treated as literal outcomes—“I opened the window, and the world pressed its cheek to the glass.”—because the choice optimized for rhythm and surprise, not for physical affordances.

Tense and POV drift come from the same rhythm-first selection and shallow coreference. The model carries multiple stylistic threads and will opportunistically switch to the one that best completes the cadence of the sentence, even if it fractures time and person. Hence the unsignaled shift from “I was an underachieving mage who had failed every glamour exam…” to “I unstrap the keystone from my pack…,” or the abrupt camera flip from “An urbane calligrapher named Sorin…” to “I set the story stick…”. Pronouns inherit from the wrong discourse thread, giving us traps like “A child struck a brass bell lightly,” followed a beat later by “She wrote first, then he,” where antecedents slide between the child and the lovers. The effect is a voice that feels musical but drifts under scrutiny because there is no discourse-level anchor enforcing consistency of narrator, timeline, or referent.

World-model and affordance gaps show up whenever concrete constraints enter the scene. The model borrows domain nouns for texture but misapplies what they can do, because there’s no physics gate to block impossible actions. In underwater sequences the rule is asserted and broken within lines: “Keep it, I write on a slate, because we cannot speak here;” is followed elsewhere by “I spoke aloud the thing I had avoided for years, crisp as the air inside my mask.”; “An apple knocked loose, drifted toward me, and I bit it” ignores the regulator still in place. In tool use, lenses polish and also self-emit light, or clockwork becomes structural engineering; the system lacks affordance rules to refuse “Each dawn, I polished the big observation window with yellow lantern glass” and later “I lit the room with the yellow lantern glass,” or to reject scale errors like “The cuckoo clock weight will counterbalance the first span.”

When closing loops, end-of-sequence pressures and a reward for neatness push it toward contrived causality. The model selects the shortest path to resolution tags, skipping setup and consequence modeling. That’s why a late “chip” suddenly reconfigures a city—“He said he needed a guarantee… the rafts realigned like ribs”—or why “Proof arrived… The letter inside was blank” is mis-labeled as sufficient evidence. It also explains why urgent windows accommodate leisurely ceremony and paperwork, and why guardians ignore core duties mid-crisis. Across these failures is a single pattern: scene stitching after implicit jumps is not instantiated with causal bridges. Constraints, once introduced, are treated as optional when a fresher image or a cleaner cadence beckons. Without an entity ledger, temporal locks, and domain affordance gates, gpt-5-medium’s stylistic strengths become liabilities—its fluency hides the seam until the logic snaps.