glm-4-5 writes at the sentence level, not at the scene level. It picks the next vivid claim even when that claim erases what the story already decided. After a descriptive beat or time reset, it re-derives reality locally and overwrites state: a dead heir reappears because “the suspended prince awaited,” a frozen mirror-lake suddenly shows “its surface rippling despite the winter chill,” the office that was a circus train becomes “Inside, the big top's remnants served as conference rooms,” and Eliza is renamed mid-page as “Today, Elima walked through the crumbling gates.” These aren’t isolated slips; they cluster after soft resets—montage paragraphs, dawn/dusk pivots, or object handoffs—when the model’s attention to prior tokens fades and a fresh, high-probability continuation wins. In practice, entity and world constraints are not persisted, so props teleport, times-of-day collide, and identities blur. You can see the same mechanism in miniature in “he heard it anyway” arriving a breath after birdsong “would never come.” The local sentence completes a pattern; the longer story no longer anchors it.

When pressed to resolve, the model fills the causal gap with template-shaped outcomes or ornamental jargon. The closing paragraph is the danger zone: “With fingers trembling but determined, Loran input the coordinates…” conjures a cosmic fix with no mechanism; “He lit the first section” follows a discovered formula that is never prepared; “As dawn approached the following day, Elena's reconstruction of that night led to a confession” reports an outcome rather than dramatizing the steps; “The merge had been successful” declares victory without showing the decisive beat. This stems from pattern-completion bias under length pressure: once the decoder recognizes “climax → resolution,” it emits the resolution scaffold (result, lesson, epiphany) and backfills pseudo-causal phrases (“sequence,” “coordinates,” “broadcast”) instead of grounded action → reaction → consequence. The same compression flips earlier premises, causing hard contradictions like “Thirty minutes remained until the tide returned” inside a story that previously calibrated a 42‑minute window and promised a return “tomorrow.”

The model also literalizes figurative language and misapplies domain terms when imagery intensifies. It treats a poetic setup as license to bend physics: “Without releasing her breath,” the character is somehow “whispering,” lunar scenes admit “the bustling sounds of the approaching mining team,” and the same line calls a pressure wave “audible … not as sound but as vibration.” Ritual-science blends yield objects like “The brass sundial began to chime midnight” in a sky-less realm and optics like “refracted cosmic radiation” visible through three miles of ocean. These errors arise when the style scorer outvotes the coarse world-knowledge check: the model prefers dense, evocative phrasing over constraints unless the prompt enforces rules. As with continuity, the failure spikes at emotion or spectacle beats—end of a reveal, a magical act—where it merges multiple high-probability idioms into oxymorons such as “The swirling hush grew louder,” or contradictory emotional states like “his voice trembling with a calm.”

A separate class of failures is editorial and encoding rubble. Under transition stress (paragraph breaks, em dashes, list-like appositions), bilingual shards and partial tokens leak into the text: “leaf litter whispering across floors sti>,” “They climbed … crevasses,沟通推进,” “Her movements … guided by breadcrumbs left by the的记忆 Keepers,” and “…toward theحراج' guarded perimeter.” Sometimes it even surfaces editorial notes: “Elara最后看着棋盘… [回注：这是中文，需要修正].” These are not mere typos; they reflect tokenizer spillover and mixed-script vocabulary from training, plus the absence of a cleanup pass that would catch truncations, unmatched punctuation, or non‑Latin intrusions. The distribution matches decoder strain points—ends of sentences, high-entropy phrases, or when the model is juggling multiple registers.

Finally, when the theme swells, glm-4-5 drifts from the scene’s focalizer into generic moralizing, and the language itself becomes the payload. It slides person and register to fit a stock summation—“to become visible to those who needed you most”—or compresses a complex arc into an abstract thesis. This same drive for lyrical density fuses incompatible images and muddles deictic anchors, encouraging POV jitter and state reassignment. The mechanisms are shared: a short planning horizon, attention decay after scenic resets, and a style-over-substance prior that rewards novel, self-contained sentences. The net effect is prose that sounds right line by line but can’t maintain who, where, when, or how across the beats that readers use to believe a story.