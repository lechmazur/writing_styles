o3-pro-medium writes smooth, image‑rich sentences but fails to keep a scene‑level ledger of what’s true. The decoder tends to optimize for local topical continuity and novelty rather than constraint satisfaction, so physical and logical invariants drift across clauses. A terrace hung from “balloons of glass” becomes “as if rubber remembered cradle songs,” and a relic is “pressed the relic beneath his tongue” even though the text made it fist‑sized. These are not isolated typos; they’re what happens when local image selection outruns a planner that would reconcile scale, material, and medium.

Medium and physics coherence are the most frequent casualties. The model freely co‑activates sensory verbs and confrontation tropes without checking the governing medium: in a flooding scene, a character “He demanded override codes” while “bubbles spilled from his lips.” The same bias toward fluent phrasing gives us “used sonar pinging” “through uncharted vacuum,” “Every exhale risked drawing acid vapor into her lungs,” and air that “smelled of burnt helium.” The mechanism is straightforward: there’s no persistent medium ledger, so underwater/space/air constraints aren’t reinstated after a beat; the next clause is steered by salient discourse cues (anger, urgency, exploration) rather than by whether sound, breath, or sonar can exist in that environment.

Object identity and role tracking decay for the same reason. When the scene pivots, the model keeps repeating the salient noun rather than the same instance, so props teleport, merge, or swap materials. We see “pocketed the wax cylinder” followed by “Behind her, the record spun,” and a central artifact shifts from “tarnished silver” to “ancient steel.” Even number/uniqueness drifts under rhetorical pressure: “the last monarch butterfly broke from the swarm.” The trigger is usually a fast cut or a summary sentence; because entity bindings aren’t maintained across clauses, the decoder fills with the nearest on‑topic token rather than emitting the transfer verb or bridge (“handed,” “replaced,” “set back on the turntable”) that would preserve state.

Time and rule anchoring fail the same way. Lyric markers are chosen for rhythm and theme, then literalized into rules that later clauses ignore. A story opens with “One rainy dawn” and later declares “dawn for the first time in years.” A paragraph establishes “at dawn,” then swings to “Tonight the bubbles felt closer.” An absolute is stated—“a hinge no exit follows”—and a beat later the character is “emerging onto the viaduct.” These contradictions arise when a high‑level summarization step deletes the causal link or exception clause; the model then prefers a stylistically consistent refrain or a new, vivid beat over re‑instantiating the earlier constraint.

When pressure to resolve rises, the model leans on plausible‑sounding but causally thin jargon and mixed metaphors, compounding the drift. Dead infrastructure revives because “pumps long thought dead coughed awake” with the help of “the sugar-water’s viscosity,” and a remote hack succeeds because she “patched her curated counter-signal through subsonic frequencies.” Metaphor stacks without a governing frame, yielding lines like “ice that refuses integers.” On top of that, the narrative mode wobbles: direct speech arrives unquoted—“I am here to restore trust after silence, she declared”—and tense splices like “slows to a stop, Silas Morel listened” jar the timeline. Category errors (“Painted lions… chipped hooves”) round out the pattern. Across all of these, the shared mechanism is a short planning horizon and style‑over‑substance bias: the decoder reliably picks the next vivid, thematically apt token, but without a persistent world model, temporal anchor, or entity map, each pick can contradict the last when the scene skips, the register compresses, or the climax demands a clean fix.