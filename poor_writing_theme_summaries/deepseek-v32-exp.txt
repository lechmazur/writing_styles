Deepseek-v32-exp’s worst failures come from a short planning horizon paired with a strong “poetic closure” bias: it optimizes each new sentence to sound definitive, symbolic, and resolved, but it doesn’t reliably re-check that sentence against the story’s current state. That’s why its highest-severity work so often collapses basic continuity in the very moments meant to feel conclusive. The model will declare an absolute rule and then immediately reach for a satisfying sensory button that violates it, as in “when noise became impossible…” followed by “The tuning fork case closed with a soft click…”. The same mechanism produces object flip‑flops where symbolic props are treated like free-floating closure tokens rather than tracked entities: “The lacquer box, now empty…” later becomes “placing the matchbox inside,” and “the matrix chip now a part of him” becomes “in his pocket.” These aren’t random typos; they look like a missing world-state ledger. Once a line like “purpose fulfilled” is emitted, the decoder keeps moving toward an ending cadence, even if it has to overwrite what it just said.

That state-tracking weakness generalizes beyond props into time, place, and physics because the model treats anchoring details as mood paint rather than constraints. It will set a scene in one season and then chase more evocative markers without paying the cost of a transition, yielding sequences like “autumn leaves” → “winter's snow… into spring's first buds” → back to “autumn light.” It also frequently asserts a metaphysical condition (“time-frozen,” “void,” “friction disappeared”) and then narrates ordinary actions that require the forbidden condition not to hold, because ordinary action beats are highly probable continuations. You see this in the SF-ish passages where sound appears in vacuum (“soft clicks,” “a perpetual whisper”), or in bodily survival errors like “He spent days in the library, his breath bubbling in the cold” with no mechanism for being underwater for days. The boundary condition is predictable: the more “absolute” and high-concept the premise (“impossible,” “eternal,” “forever”), the more likely the model is to break it later when it reaches for familiar dramatic beats like a click, a whisper, a final star, a deep breath.

The same local-optimization habit drives its causality failures. When the story needs to pivot—especially under tight length budgets—it substitutes explanation-shaped sentences for an actual chain of operations, so resolutions feel like deus ex rather than consequences. You get instant “perfect fit” climaxes and single-action cosmic repairs such as “clicked into place with a soft, resonant chime… stitching the frayed edges of reality,” or proofs that materialize as declarations: “Time... it really does move sideways.” Mystery logic often becomes a jump cut from a token clue to a fully specified conclusion, like the ribbon inference that leaps to “not torn in a struggle… Someone she knew. Someone who had helped her…” without any on-page reasoning. Even when it gestures at a “method,” the method is often nonfunctional or temporally incompatible with the scene’s urgency, as in “We need to reinforce the signal,” solved by seeds “activated only when grown in specific clusters.” Mechanistically, the model is pattern-matching to familiar narrative templates (“artifact clicks → reality repaired,” “clue → reveal”) and then compressing away the middle because the middle is harder: it requires maintaining intermediate state, showing constraints, and committing to testable steps.

A third failure mode is voice intrusion: when the model is stressed by needing to explain stakes or wrap an arc, it slides into outline language that reads like internal notes rather than lived narration. Lines such as “His attribute of being shyly unstoppable…” and “The character of Elian… The setting… The timeframe…” are not just awkward; they reveal a compression strategy where the model labels story components instead of dramatizing them. This also explains why it leans on epiphany verbs and meta abstractions (“progressive disclosure,” “nested awakenings,” “motivation was clear”) to paper over missing causal links. The result is that climaxes arrive as summaries of transformation rather than transformations the reader can track, so emotional payoff feels unearned even when the prose is trying to sound profound.

Finally, when these pressures coincide—high-concept rule, looming climax, need for thematic resonance—deepseek-v32-exp often degrades at the sentence level. It stacks abstractions and nominalizations until grammar and meaning slip, producing phrases like “after the missing return changed,” “this method by the hum…,” or malformed morphology such as “As he subsume…”. At the same time, it blurs ontology: it declares something nonphysical and then makes it tactile, as in “not physical objects but states of awareness” followed by “her fingers brushing,” or it contradicts itself about whether time is objective (“a bubble…”) versus purely subjective (“Time didn't slow objectively”). These aren’t separate problems; they’re the same mechanism expressed in different layers. The model prioritizes cadence, motif, and closure over constraint satisfaction, and without an explicit internal checklist for world rules, object locations, and causal bridges, it reaches for whichever continuation best completes the paragraph—even if that completion breaks the story’s own reality.